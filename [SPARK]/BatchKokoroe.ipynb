{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 173 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" % \"spark-core_2.10\" % \"1.6.0\",\n",
    "    \"org.apache.spark\" % \"spark-sql_2.10\" % \"1.6.0\",\n",
    "    \"com.datastax.spark\" %% \"spark-cassandra-connector-java\" % \"1.6.0-M1\",\n",
    "    \"io.continuum.bokeh\" %% \"bokeh\" % \"0.6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{ SparkConf, SparkContext }\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.DataFrame\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{ SparkConf, SparkContext }\n",
    "import org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mHOST\u001b[0m: String = \u001b[32m\"ns370799.ip-91-121-193.eu\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val HOST = \"ns370799.ip-91-121-193.eu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/04/04 23:11:24 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/04/04 23:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/04/04 23:11:25 INFO SecurityManager: Changing view acls to: rico\n",
      "16/04/04 23:11:25 INFO SecurityManager: Changing modify acls to: rico\n",
      "16/04/04 23:11:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(rico); users with modify permissions: Set(rico)\n",
      "16/04/04 23:11:27 INFO Utils: Successfully started service 'sparkDriver' on port 56577.\n",
      "16/04/04 23:11:29 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/04/04 23:11:29 INFO Remoting: Starting remoting\n",
      "16/04/04 23:11:30 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@91.121.193.238:48020]\n",
      "16/04/04 23:11:30 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 48020.\n",
      "16/04/04 23:11:30 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/04/04 23:11:30 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/04/04 23:11:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-308150ed-ea22-4488-9522-c3656a787657\n",
      "16/04/04 23:11:30 INFO MemoryStore: MemoryStore started with capacity 429.8 MB\n",
      "16/04/04 23:11:30 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/04/04 23:11:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/04/04 23:11:31 INFO SparkUI: Started SparkUI at http://91.121.193.238:4040\n",
      "16/04/04 23:11:32 INFO AppClient$ClientEndpoint: Connecting to master spark://ns370799.ip-91-121-193.eu:7077...\n",
      "16/04/04 23:11:32 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160404231132-0006\n",
      "16/04/04 23:11:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54592.\n",
      "16/04/04 23:11:32 INFO NettyBlockTransferService: Server created on 54592\n",
      "16/04/04 23:11:32 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/04/04 23:11:32 INFO BlockManagerMasterEndpoint: Registering block manager 91.121.193.238:54592 with 429.8 MB RAM, BlockManagerId(driver, 91.121.193.238, 54592)\n",
      "16/04/04 23:11:32 INFO BlockManagerMaster: Registered BlockManager\n",
      "16/04/04 23:11:33 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[0m: org.apache.spark.SparkConf = org.apache.spark.SparkConf@580f304b\n",
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@63e35093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val conf = new SparkConf()\n",
    ".setAppName(\"Integration Batch Kokoroe\")\n",
    ".setMaster(s\"spark://$HOST:7077\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msqlContext\u001b[0m: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@440ce99f"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addPath(\"/home/rico/archives/mysql-connector-java-5.1.38.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mselectAll\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "* param = table: String name of the table\n",
    "* return = df: DataFrame\n",
    "**/\n",
    "def selectAll (table :String ) :DataFrame = {\n",
    "     (sqlContext.read.format(\"jdbc\")\n",
    "    .option(\"url\", s\"jdbc:mysql://$HOST:3306/kokoroe\")\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "    .option(\"dbtable\", table)\n",
    "    .option(\"user\", \"root\").load())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val coursesRdd = selectAll(\"courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
